{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b8aca8fa-191d-4294-b199-b7fda4d55b3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8aca8fa-191d-4294-b199-b7fda4d55b3d",
        "outputId": "de9d31ea-2383-4399-929e-e836b7f60d07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import string\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import math\n",
        "import tqdm\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Current device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "58b76b4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, max_len=10, vocab_size=30000):\n",
        "        self.file_path = file_path\n",
        "        self.max_len = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = ByteLevelBPETokenizer()\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        self._train_tokenizer()\n",
        "        self._load_tokenizer()\n",
        "        self.tokens_num = self._tokenize_text()\n",
        "        self.vocab_size = self.tokenizer.get_vocab_size()\n",
        "        self.sentence, self.label = self._create_sequences(self.tokens_num)\n",
        "\n",
        "    def _train_tokenizer(self):\n",
        "        self.tokenizer.train(files=[self.file_path], vocab_size=self.vocab_size, min_frequency=2)\n",
        "\n",
        "    def _load_tokenizer(self):\n",
        "        self.tokenizer.save_model(\".\", \"bpe\")\n",
        "        self.tokenizer = ByteLevelBPETokenizer(\n",
        "            \"./bpe-vocab.json\",\n",
        "            \"./bpe-merges.txt\",\n",
        "        )\n",
        "\n",
        "    def _tokenize_text(self):\n",
        "        with open(self.file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text = file.read().replace(\"\\n\", \" \").lower()\n",
        "        text = ''.join(char for char in text if char not in string.punctuation)\n",
        "        tokens = self.tokenizer.encode(text).ids\n",
        "        return tokens\n",
        "\n",
        "    def _create_sequences(self, tokens_num):\n",
        "        x = []\n",
        "        y = []\n",
        "        for i in range(len(tokens_num) - self.max_len - 1):\n",
        "            x.append(tokens_num[i:self.max_len + i])\n",
        "            y.append(tokens_num[self.max_len + i])\n",
        "        sentence = torch.Tensor(x).long()\n",
        "        label = torch.Tensor(y).long()\n",
        "        return sentence, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentence)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentence[idx], self.label[idx]\n",
        "\n",
        "    def decode(self, token):\n",
        "        token = token.cpu().numpy()\n",
        "        return self.tokenizer.decode(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "63ba620d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Vocab size: 12711\n",
            "Batch 1:\n",
            "Sentences: tensor([[ 3785,   278,   258,   285,   490,   833,   622,  1420,   705,   324],\n",
            "        [  296,  7463,   362,  5360,  1056,   267,  1940,  2047,   300,  3770],\n",
            "        [  258,  2719,  7328,  6684,   300,  2967,   287,  3169,   450,    67],\n",
            "        [  307,  1359,  2308,  8742,   434,  1420,   466,   321,  1058, 10501],\n",
            "        [ 1535,   287,   267,  3588,   418,   553,   403,  1059,  1301,  2559],\n",
            "        [ 2890,    78,   342,  3153,  9714,  1076,   755,   342,  3023,   267],\n",
            "        [  629,  1420,   821,   327,   323,  2333,  1940,   220,   843,   299],\n",
            "        [  338,   220,   282,  1075,  1770,   533,   516,   306,  1716,  5702],\n",
            "        [ 2375,   220,  1099,  6922,  2716,   415,   306,  3617,   267,  2375],\n",
            "        [  523,   772,  1315,   704,   317,   307,   267,  1066,   616,   321],\n",
            "        [ 1420,   493,   422,   272,   258,  5244,   220,   281,    84,  1691],\n",
            "        [   67,   286,   503,   267,  1442,   574,   447,   327,   495,   267],\n",
            "        [ 8137,  1830,   460,   337,  4280,   907,   292,  6265,   315,   578],\n",
            "        [  598,   357, 10414,   314,   220,  8433,  2716,   267,  1389,    82],\n",
            "        [ 2954,  2743,   387,   537,   317,   577,   324,   362,   412, 10885],\n",
            "        [  964,   296,  1789, 10492,   317,   307,   347,  1361,   387,  4147],\n",
            "        [  347,  3436,  3353,  1280,   487,   323,   418,   267,  1218,   300],\n",
            "        [ 9544,   327,   677,   258,   562,   366,  3475,   258,  1373,   296],\n",
            "        [  523,  2278,   220,   783,   279,  1136,   321,  5006,   287,   267],\n",
            "        [  615,   269,   513,   447,   277,  1957,  1036,   745,   287,   355],\n",
            "        [  484,  6754,   527,   329,  8636,  1141,   296,   578,   258,  1725],\n",
            "        [  412,  4213, 10323,  3534,    82,   287,  5369,   276,   313,  1867],\n",
            "        [ 1185,   220,  1144,  4414,   306,  1185,   384,   772,  1953,   434],\n",
            "        [  338,   324,   321,  2593,  5894,   495,   220,   289,  2265,  1420],\n",
            "        [ 3233,  2881,   528,  1789,   304,  2428,   286,  1614,  4458,   362],\n",
            "        [ 1823,   323,   307,  4971,   300,   317,   292,  2080,  1303,  2280],\n",
            "        [  929,   929,   434,   355,   288,   840,  2037,   267,  2117,   383],\n",
            "        [ 4146,   434,   288,   418,  3075,   307,   267,  1296,  1442,   323],\n",
            "        [  505,  3105,   220,  1603,    76,   619,  1073,  1826,   342,  1961],\n",
            "        [  743,  1359,   313,   361,  8669,   357,   220,   302, 10257,   338],\n",
            "        [  447,   460,   220,   289,  4008,   362,   324,   258,  2179,  1466],\n",
            "        [  258,   590,  4504,   287,  4759,   288,   296,  1013,   756,  1432]])\n",
            "Labels: tensor([ 258,  338, 4185,  292, 7376, 1624,  619,  365,  550, 5661,  296, 1860,\n",
            "         258, 1362, 1436,  220,  338, 1380, 3613,  360,  275, 2211, 1420,  516,\n",
            "         324, 1508, 5127,  395,  321,  324,  220,  288])\n",
            "First sentence in the batch:  professes a hothouse which i think is\n"
          ]
        }
      ],
      "source": [
        "dataset = TextDataset(\"/home/kunalkushwahatg/Desktop/transformer_from_scratch/data/input.txt\", max_len=10, vocab_size=30000)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(f\"Vocab size: {dataset.vocab_size}\")\n",
        "\n",
        "for batch_idx, (sentences, labels) in enumerate(dataloader):\n",
        "    print(f\"Batch {batch_idx+1}:\")\n",
        "    print(\"Sentences:\", dataset.decode(sentences))\n",
        "    print(\"Labels:\", labels)\n",
        "    print(\"First sentence in the batch:\", dataset.decode(sentences[0]))\n",
        "    if batch_idx == 0:\n",
        "        break "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2e3f3c3b-2c07-4e01-b76c-25153e5a7143",
      "metadata": {
        "id": "2e3f3c3b-2c07-4e01-b76c-25153e5a7143"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    '''\n",
        "    Converts the vector embedding of a batch of sequences to their positional encoding vectors.\n",
        "\n",
        "    Arguments:\n",
        "            shape : shape of embedding vector => tuple(batch_size, max_len, dmodel)\n",
        "            device : device to perform the computation on (e.g., 'cpu' or 'cuda')\n",
        "\n",
        "    Returns:\n",
        "            positional encoded vector\n",
        "\n",
        "    '''\n",
        "    def __init__(self, shape, device='cpu'):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.max_len = shape[1]\n",
        "        self.dmodel = shape[2]\n",
        "        self.device = device\n",
        "\n",
        "        position = torch.arange(0, self.max_len, device=self.device).float().unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, self.dmodel, 2, device=self.device).float() * -(math.log(10000.0) / self.dmodel))\n",
        "\n",
        "        pos_enc = torch.zeros((1, self.max_len, self.dmodel), device=self.device)\n",
        "        pos_enc[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pos_enc[0, :, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.pos_enc = pos_enc\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_enc[:, :x.size(1), :]\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "2738ce16-3467-4300-b3bb-92442eadaca1",
      "metadata": {
        "id": "2738ce16-3467-4300-b3bb-92442eadaca1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    '''\n",
        "    Multi-Head Attention mechanism for transformer models.\n",
        "\n",
        "    Arguments:\n",
        "        dmodel: Dimension of the model\n",
        "        heads: Number of attention heads\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Perform multi-head attention on the input tensor x\n",
        "    '''\n",
        "    def __init__(self, dmodel, heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.dmodel = dmodel\n",
        "        self.heads = heads\n",
        "        self.head_size = dmodel // heads\n",
        "\n",
        "        self.k_linear = nn.Linear(dmodel, dmodel)\n",
        "        self.q_linear = nn.Linear(dmodel, dmodel)\n",
        "        self.v_linear = nn.Linear(dmodel, dmodel)\n",
        "        self.out_linear = nn.Linear(dmodel, dmodel)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        '''\n",
        "        Split the last dimension into (heads, head_size) and transpose to shape (batch_size, heads, seq_len, head_size).\n",
        "        '''\n",
        "        return x.view(batch_size, -1, self.heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "    def attention(self, k, q, v):\n",
        "        '''\n",
        "        Compute the attention weights and apply them to the value vectors.\n",
        "        '''\n",
        "        d_k = q.size(-1)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32, device=q.device))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        return torch.matmul(attn, v)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Perform the multi-head attention mechanism on the input tensor x.\n",
        "        '''\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        K = self.split_heads(self.k_linear(x), batch_size)  # Key: What can I offer\n",
        "        Q = self.split_heads(self.q_linear(x), batch_size)  # Query: What am I looking for\n",
        "        V = self.split_heads(self.v_linear(x), batch_size)  # Value: What I actually offer\n",
        "\n",
        "        attn_output = self.attention(K, Q, V)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.dmodel)\n",
        "\n",
        "        return self.out_linear(attn_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fd3a1554-fac1-45a2-a86a-9d817e7b0eb3",
      "metadata": {
        "id": "fd3a1554-fac1-45a2-a86a-9d817e7b0eb3"
      },
      "outputs": [],
      "source": [
        "class AddAndNorm(nn.Module):\n",
        "    '''\n",
        "    Add and Layer Normalization module for transformer models.\n",
        "\n",
        "    Arguments:\n",
        "        dmodel: Dimension of the model\n",
        "\n",
        "    Methods:\n",
        "        forward(x, residual): Add the input tensor x and the residual tensor, then apply layer normalization\n",
        "    '''\n",
        "    def __init__(self, dmodel):\n",
        "        super(AddAndNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(dmodel)\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        '''\n",
        "        Add the input tensor x and the residual tensor, then apply layer normalization.\n",
        "\n",
        "        Arguments:\n",
        "            x: Input tensor\n",
        "            residual: Residual tensor to be added to the input tensor\n",
        "\n",
        "        Returns:\n",
        "            Tensor after addition and layer normalization\n",
        "        '''\n",
        "        return self.layer_norm(x + residual)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3abf7b27-898e-4eca-a720-1da2abea3419",
      "metadata": {
        "id": "3abf7b27-898e-4eca-a720-1da2abea3419"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    '''\n",
        "    Position-wise Feed-Forward Network for transformer models with dropout.\n",
        "\n",
        "    Arguments:\n",
        "        dmodel: Dimension of the model\n",
        "        dropout: Dropout probability\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Apply the feed-forward network with dropout on the input tensor x\n",
        "    '''\n",
        "    def __init__(self, dmodel, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(dmodel, dmodel)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dmodel, dmodel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Apply the feed-forward network with dropout on the input tensor x.\n",
        "\n",
        "        Arguments:\n",
        "            x: Input tensor\n",
        "\n",
        "        Returns:\n",
        "            Tensor after applying the feed-forward network and dropout\n",
        "        '''\n",
        "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "a997cda3-7168-4fe7-994a-48fb53bd0ed6",
      "metadata": {
        "id": "a997cda3-7168-4fe7-994a-48fb53bd0ed6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Transformer Encoder implementation.\n",
        "\n",
        "    Arguments:\n",
        "        vocab_size: Size of the vocabulary\n",
        "        shape: Shape of the input tensor (batch_size, max_len, dmodel)\n",
        "        heads: Number of attention heads\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Forward pass through the encoder\n",
        "    '''\n",
        "    def __init__(self, vocab_size, shape, heads=4):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, shape[2])\n",
        "        self.positional_encoding = PositionalEncoding(shape,device=device)\n",
        "        self.multi_headed_attention = MultiHeadAttention(shape[2], heads)\n",
        "        self.add_and_norm1 = AddAndNorm(shape[2])\n",
        "        self.feed_forward = FeedForward(dmodel=shape[2])\n",
        "        self.add_and_norm2 = AddAndNorm(shape[2])\n",
        "        self.linear = nn.Linear(shape[2], 512)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.token_embedding_table(x)\n",
        "        residual = self.positional_encoding(out)\n",
        "        out = self.multi_headed_attention(residual)\n",
        "\n",
        "        residual = self.add_and_norm1(out, residual)\n",
        "\n",
        "        out = self.feed_forward(residual)\n",
        "        out = self.add_and_norm2(out, residual)\n",
        "\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "1fed33ce",
      "metadata": {
        "id": "1fed33ce"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Pretraining(nn.Module):\n",
        "    '''\n",
        "    Pretraining model for next word prediction using a transformer encoder.\n",
        "\n",
        "    Arguments:\n",
        "        vocab_size: Size of the vocabulary\n",
        "        shape: Shape of the input tensor (batch_size, max_len, dmodel)\n",
        "        heads: Number of attention heads\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Forward pass through the pretraining model\n",
        "        predict_next_word(x): Predict the next word for the input sequence\n",
        "    '''\n",
        "    def __init__(self, vocab_size, shape, heads=4):\n",
        "        super(Pretraining, self).__init__()\n",
        "        self.encoder = Encoder(vocab_size, shape, heads)\n",
        "        self.linear = nn.Linear(shape[2] * shape[1], vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encoder(x)\n",
        "        out = out.view(out.size(0), -1) #torch.Size([Batch,time*dmodel])\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "5b97bbdd-127d-495e-b32c-ab099312f78b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b97bbdd-127d-495e-b32c-ab099312f78b",
        "outputId": "0ddd68de-957c-4945-f91f-874c1792a6f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6259/6259 [01:49<00:00, 57.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Loss: 6.145278875735225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6259/6259 [01:49<00:00, 57.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Average Loss: 5.546202123688897\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6259/6259 [01:49<00:00, 57.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Average Loss: 4.801016961448317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6259/6259 [01:49<00:00, 57.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Average Loss: 4.067565264879352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6259/6259 [01:49<00:00, 57.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Average Loss: 3.4987538750321363\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6259/6259 [01:48<00:00, 57.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Average Loss: 3.064599224145581\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6259/6259 [01:48<00:00, 57.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7, Average Loss: 2.7542555458840137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6259/6259 [01:48<00:00, 57.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, Average Loss: 2.508637972791501\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6259/6259 [01:49<00:00, 57.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, Average Loss: 2.2968764330826925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6259/6259 [01:48<00:00, 57.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10, Average Loss: 2.127277320919564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = Pretraining(vocab_size,shape)\n",
        "criterition = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "import random\n",
        "random.shuffle(batch)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "    losses = []\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for b in tqdm.tqdm(batch):\n",
        "        inputs, targets = b[0].to(device), b[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        res = model(inputs)\n",
        "        loss  = criterition(res ,targets)\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        losses.append(loss)\n",
        "\n",
        "    # Calculate and print the average loss for the epoch\n",
        "    average_loss = running_loss / len(batch)\n",
        "    print(f'Epoch {epoch + 1}, Average Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "b8c69d0b",
      "metadata": {
        "id": "b8c69d0b"
      },
      "outputs": [],
      "source": [
        "#softmax res\n",
        "res = nn.Softmax(dim=-1)(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "id": "Fml8-Hy9s_Jj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fml8-Hy9s_Jj",
        "outputId": "cf4a659e-9131-4d29-e88e-9b18727e45b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "it\n"
          ]
        }
      ],
      "source": [
        "sentence = 'we welcome to the book of writing making working can'\n",
        "tokens = sentence.split(\" \")\n",
        "tokens_num = []\n",
        "for i in tokens:\n",
        "    tokens_num.append(vocab_to_idx[i])\n",
        "\n",
        "out = model(torch.tensor(tokens_num).unsqueeze(0).to(device))\n",
        "out = nn.Softmax(dim=-1)(out)\n",
        "out = torch.argmax(out,dim=-1).item()\n",
        "print(idx_to_vocab[out])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "qgr8j9d64fEJ",
      "metadata": {
        "id": "qgr8j9d64fEJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file '/home/kunalkushwahatg/Desktop/transformer_from_scratch/notebook/dataset.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc84c30b",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
