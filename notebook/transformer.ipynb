{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "QfCD8GVoKGAF"
      },
      "id": "QfCD8GVoKGAF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kunalkushwahatg/transformer_from_scratch.git\n",
        "!cd transformer_from_scratch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgMxXKV15Jw5",
        "outputId": "9bdf1ca1-ca48-45e5-848e-d9a976f2d9a8"
      },
      "id": "UgMxXKV15Jw5",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformer_from_scratch'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 52 (delta 10), reused 41 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (52/52), 580.68 KiB | 7.00 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b8aca8fa-191d-4294-b199-b7fda4d55b3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8aca8fa-191d-4294-b199-b7fda4d55b3d",
        "outputId": "02aa0d0f-20e0-4875-e21c-1de27e418659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import string\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import math\n",
        "import tqdm\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sentencepiece as spm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Current device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load configurations\n",
        "DEVICE = device\n",
        "BATCH_SIZE = 128\n",
        "MAX_LEN = 10\n",
        "DMODEL = 512\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "VOCAB_SIZE = 10000\n"
      ],
      "metadata": {
        "id": "X-dyeVI-7W4J"
      },
      "id": "X-dyeVI-7W4J",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "58b76b4b",
      "metadata": {
        "id": "58b76b4b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, max_len, vocab_size):\n",
        "        self.file_path = file_path\n",
        "        self.max_len = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.sp_model_path = 'spm.model'\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        self._train_tokenizer()\n",
        "        self._load_tokenizer()\n",
        "        self.tokens_num = self._tokenize_text()\n",
        "        self.sentence, self.label = self._create_sequences(self.tokens_num)\n",
        "        self.vocab_size = self._get_vocab_size()\n",
        "\n",
        "    def _train_tokenizer(self):\n",
        "        spm.SentencePieceTrainer.train(\n",
        "            input=self.file_path, model_prefix='spm', vocab_size=self.vocab_size,\n",
        "            pad_id=0, unk_id=1, bos_id=2, eos_id=3, user_defined_symbols=['<pad>', '<bos>', '<eos>']\n",
        "        )\n",
        "\n",
        "    def _load_tokenizer(self):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.load(self.sp_model_path)\n",
        "\n",
        "    def _tokenize_text(self):\n",
        "        with open(self.file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text = file.read().replace(\"\\n\", \" \").lower()\n",
        "        text = ''.join(char for char in text if char not in string.punctuation)\n",
        "        tokens = self.sp.encode(text)\n",
        "        return tokens\n",
        "\n",
        "    def _create_sequences(self, tokens_num):\n",
        "        x = []\n",
        "        y = []\n",
        "        for i in range(len(tokens_num) - self.max_len - 1):\n",
        "            x.append(tokens_num[i:self.max_len + i])\n",
        "            y.append(tokens_num[self.max_len + i])\n",
        "        sentence = torch.Tensor(x).long()\n",
        "        label = torch.Tensor(y).long()\n",
        "        return sentence, label\n",
        "\n",
        "    def _get_vocab_size(self):\n",
        "        return self.sp.get_piece_size()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentence)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentence[idx], self.label[idx]\n",
        "\n",
        "    def decode(self, token):\n",
        "        token = token.cpu().numpy()\n",
        "        return self.sp.decode(token.tolist())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "63ba620d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63ba620d",
        "outputId": "a86eb371-4bb5-481e-d707-bdd2001ecd51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10000\n"
          ]
        }
      ],
      "source": [
        "dataset = TextDataset(\"/content/transformer_from_scratch/data/input.txt\", max_len=MAX_LEN, vocab_size=VOCAB_SIZE)\n",
        "print(\"Vocabulary size:\", dataset.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "2e3f3c3b-2c07-4e01-b76c-25153e5a7143",
      "metadata": {
        "id": "2e3f3c3b-2c07-4e01-b76c-25153e5a7143"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    '''\n",
        "    Converts the vector embedding of a batch of sequences to their positional encoding vectors.\n",
        "\n",
        "    Arguments:\n",
        "            shape : shape of embedding vector => tuple(batch_size, max_len, dmodel)\n",
        "            device : device to perform the computation on (e.g., 'cpu' or 'cuda')\n",
        "\n",
        "    Returns:\n",
        "            positional encoded vector\n",
        "\n",
        "    '''\n",
        "    def __init__(self, shape, device='cpu'):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.max_len = shape[1]\n",
        "        self.dmodel = shape[2]\n",
        "        self.device = device\n",
        "\n",
        "        position = torch.arange(0, self.max_len, device=self.device).float().unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, self.dmodel, 2, device=self.device).float() * -(math.log(10000.0) / self.dmodel))\n",
        "\n",
        "        pos_enc = torch.zeros((1, self.max_len, self.dmodel), device=self.device)\n",
        "        pos_enc[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pos_enc[0, :, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.pos_enc = pos_enc\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_enc[:, :x.size(1), :]\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "2738ce16-3467-4300-b3bb-92442eadaca1",
      "metadata": {
        "id": "2738ce16-3467-4300-b3bb-92442eadaca1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    '''\n",
        "    Multi-Head Attention mechanism for transformer models.\n",
        "\n",
        "    Arguments:\n",
        "        dmodel: Dimension of the model\n",
        "        heads: Number of attention heads\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Perform multi-head attention on the input tensor x\n",
        "    '''\n",
        "    def __init__(self, dmodel, heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.dmodel = dmodel\n",
        "        self.heads = heads\n",
        "        self.head_size = dmodel // heads\n",
        "\n",
        "        self.k_linear = nn.Linear(dmodel, dmodel)\n",
        "        self.q_linear = nn.Linear(dmodel, dmodel)\n",
        "        self.v_linear = nn.Linear(dmodel, dmodel)\n",
        "        self.out_linear = nn.Linear(dmodel, dmodel)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        '''\n",
        "        Split the last dimension into (heads, head_size) and transpose to shape (batch_size, heads, seq_len, head_size).\n",
        "        '''\n",
        "        return x.view(batch_size, -1, self.heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "    def attention(self, k, q, v):\n",
        "        '''\n",
        "        Compute the attention weights and apply them to the value vectors.\n",
        "        '''\n",
        "        d_k = q.size(-1)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32, device=q.device))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        return torch.matmul(attn, v)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Perform the multi-head attention mechanism on the input tensor x.\n",
        "        '''\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        K = self.split_heads(self.k_linear(x), batch_size)  # Key: What can I offer\n",
        "        Q = self.split_heads(self.q_linear(x), batch_size)  # Query: What am I looking for\n",
        "        V = self.split_heads(self.v_linear(x), batch_size)  # Value: What I actually offer\n",
        "\n",
        "        attn_output = self.attention(K, Q, V)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.dmodel)\n",
        "\n",
        "        return self.out_linear(attn_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "fd3a1554-fac1-45a2-a86a-9d817e7b0eb3",
      "metadata": {
        "id": "fd3a1554-fac1-45a2-a86a-9d817e7b0eb3"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AddAndNorm(nn.Module):\n",
        "    '''\n",
        "    Add and Layer Normalization module for transformer models.\n",
        "\n",
        "    Arguments:\n",
        "        dmodel: Dimension of the model\n",
        "\n",
        "    Methods:\n",
        "        forward(x, residual): Add the input tensor x and the residual tensor, then apply layer normalization\n",
        "    '''\n",
        "    def __init__(self, dmodel):\n",
        "        super(AddAndNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(dmodel)\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        '''\n",
        "        Add the input tensor x and the residual tensor, then apply layer normalization.\n",
        "\n",
        "        Arguments:\n",
        "            x: Input tensor\n",
        "            residual: Residual tensor to be added to the input tensor\n",
        "\n",
        "        Returns:\n",
        "            Tensor after addition and layer normalization\n",
        "        '''\n",
        "        return self.layer_norm(x + residual)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "3abf7b27-898e-4eca-a720-1da2abea3419",
      "metadata": {
        "id": "3abf7b27-898e-4eca-a720-1da2abea3419"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    '''\n",
        "    Position-wise Feed-Forward Network for transformer models with dropout.\n",
        "\n",
        "    Arguments:\n",
        "        dmodel: Dimension of the model\n",
        "        dropout: Dropout probability\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Apply the feed-forward network with dropout on the input tensor x\n",
        "    '''\n",
        "    def __init__(self, dmodel, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(dmodel, dmodel)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dmodel, dmodel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Apply the feed-forward network with dropout on the input tensor x.\n",
        "\n",
        "        Arguments:\n",
        "            x: Input tensor\n",
        "\n",
        "        Returns:\n",
        "            Tensor after applying the feed-forward network and dropout\n",
        "        '''\n",
        "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "a997cda3-7168-4fe7-994a-48fb53bd0ed6",
      "metadata": {
        "id": "a997cda3-7168-4fe7-994a-48fb53bd0ed6"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Transformer Encoder implementation.\n",
        "\n",
        "    Arguments:\n",
        "        vocab_size: Size of the vocabulary\n",
        "        shape: Shape of the input tensor (batch_size, max_len, dmodel)\n",
        "        heads: Number of attention heads\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Forward pass through the encoder\n",
        "    '''\n",
        "    def __init__(self, vocab_size, shape, heads=4):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, shape[2])\n",
        "        self.positional_encoding = PositionalEncoding(shape,device=device)\n",
        "        self.multi_headed_attention = MultiHeadAttention(shape[2], heads)\n",
        "        self.add_and_norm1 = AddAndNorm(shape[2])\n",
        "        self.feed_forward = FeedForward(dmodel=shape[2])\n",
        "        self.add_and_norm2 = AddAndNorm(shape[2])\n",
        "        self.linear = nn.Linear(shape[2], 512)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.token_embedding_table(x)\n",
        "        residual = self.positional_encoding(out)\n",
        "        out = self.multi_headed_attention(residual)\n",
        "\n",
        "        residual = self.add_and_norm1(out, residual)\n",
        "\n",
        "        out = self.feed_forward(residual)\n",
        "        out = self.add_and_norm2(out, residual)\n",
        "\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "1fed33ce",
      "metadata": {
        "id": "1fed33ce"
      },
      "outputs": [],
      "source": [
        "class Pretraining(nn.Module):\n",
        "    '''\n",
        "    Pretraining model for next word prediction using a transformer encoder.\n",
        "\n",
        "    Arguments:\n",
        "        vocab_size: Size of the vocabulary\n",
        "        shape: Shape of the input tensor (batch_size, max_len, dmodel)\n",
        "        heads: Number of attention heads\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Forward pass through the pretraining model\n",
        "        predict_next_word(x): Predict the next word for the input sequence\n",
        "    '''\n",
        "    def __init__(self,n_encoders, vocab_size, shape, heads=4):\n",
        "        super(Pretraining, self).__init__()\n",
        "        self.encoder = Encoder(vocab_size, shape, heads)\n",
        "        self.linear = nn.Linear(shape[2] * shape[1], vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encoder(x)\n",
        "        out = out.view(out.size(0), -1) #torch.Size([Batch,time*dmodel])\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pretraining(n_encoders=3,vocab_size=dataset.vocab_size, shape=(BATCH_SIZE, MAX_LEN, DMODEL), heads=4)\n",
        "model.to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKAd06Du5hGP",
        "outputId": "1d791a42-8432-4e0c-c556-f354c01206d0"
      },
      "id": "xKAd06Du5hGP",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pretraining(\n",
              "  (encoder): Encoder(\n",
              "    (token_embedding_table): Embedding(10000, 512)\n",
              "    (positional_encoding): PositionalEncoding()\n",
              "    (multi_headed_attention): MultiHeadAttention(\n",
              "      (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (out_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "    )\n",
              "    (add_and_norm1): AddAndNorm(\n",
              "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (feed_forward): FeedForward(\n",
              "      (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (relu): ReLU()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "    )\n",
              "    (add_and_norm2): AddAndNorm(\n",
              "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (softmax): Softmax(dim=-1)\n",
              "  )\n",
              "  (linear): Linear(in_features=5120, out_features=10000, bias=True)\n",
              "  (softmax): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b97bbdd-127d-495e-b32c-ab099312f78b",
      "metadata": {
        "id": "5b97bbdd-127d-495e-b32c-ab099312f78b"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    losses = []\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for b in tqdm.tqdm(dataloader, desc=f'Epoch {epoch + 1}/{EPOCHS - 1}'):\n",
        "        inputs, targets = b[0].to(DEVICE), b[1].to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    average_loss = sum(losses) / len(losses)\n",
        "    print(f'Epoch {epoch + 1}, Average Loss: {average_loss}')\n",
        "\n",
        "print(\"Training completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fml8-Hy9s_Jj",
      "metadata": {
        "id": "Fml8-Hy9s_Jj"
      },
      "outputs": [],
      "source": [
        "#infrence\n",
        "TOKEN_GEN = 100\n",
        "text = \"hello this story mostly tells us about how we look \"\n",
        "\n",
        "def infrence(text):\n",
        "    model.eval()\n",
        "    for i in range(TOKEN_GEN):\n",
        "        tokens = dataset.sp.encode(text)\n",
        "\n",
        "        #acess last MAX_LEN tokens\n",
        "        tokens = tokens[-MAX_LEN:]\n",
        "\n",
        "        #convert to tensor\n",
        "        tokens = torch.Tensor(tokens).long().unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        #get prediction\n",
        "        prediction = model(tokens)\n",
        "        prediction = prediction.squeeze(0)\n",
        "\n",
        "        #get argmax\n",
        "        prediction = torch.argmax(prediction,dim=-1)\n",
        "\n",
        "        #decode\n",
        "        text += \" \" + dataset.decode(prediction)\n",
        "\n",
        "    return text\n",
        "\n",
        "print(infrence(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i3ahpOsaCUH_"
      },
      "id": "i3ahpOsaCUH_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}