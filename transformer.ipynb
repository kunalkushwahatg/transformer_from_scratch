{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunalkushwahatg/transformer_from_scratch/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fnU999-N-QTT",
      "metadata": {
        "id": "fnU999-N-QTT"
      },
      "source": [
        "Project is not yet completed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b8aca8fa-191d-4294-b199-b7fda4d55b3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8aca8fa-191d-4294-b199-b7fda4d55b3d",
        "outputId": "cc29184d-22e0-4bd4-c8a4-16a35cbb8515"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import string\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import math\n",
        "import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Current device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "62d6fd62-61cb-4740-a727-d48ee65f4abd",
      "metadata": {
        "id": "62d6fd62-61cb-4740-a727-d48ee65f4abd"
      },
      "outputs": [],
      "source": [
        "file = open(\"input.txt\",\"r\",encoding=\"utf-8\")\n",
        "text = file.read()\n",
        "text = text.replace(\"\\n\" , \" \").lower()\n",
        "punctuation_chars = string.punctuation\n",
        "text = ''.join(char for char in text if char not in punctuation_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f9fe132d-ad12-4ddc-b57c-2e507029f5d9",
      "metadata": {
        "id": "f9fe132d-ad12-4ddc-b57c-2e507029f5d9"
      },
      "outputs": [],
      "source": [
        "tokens = text.split(\" \")\n",
        "vocab = list(set(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b6f2ad48-ea79-4da8-aac3-fba00fc620a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6f2ad48-ea79-4da8-aac3-fba00fc620a0",
        "outputId": "02d7a8ba-e8a2-44e5-e0d1-2ffa7f5a3797"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12849/12849 [01:09<00:00, 185.39it/s]\n"
          ]
        }
      ],
      "source": [
        "for i in tqdm.tqdm(vocab):\n",
        "    if tokens.count(i) < 5:\n",
        "        tokens.remove(i)\n",
        "vocab = list(set(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "278f74d0-ba94-4893-90c7-1268dbb953d0",
      "metadata": {
        "id": "278f74d0-ba94-4893-90c7-1268dbb953d0"
      },
      "outputs": [],
      "source": [
        "vocab_to_idx = {}\n",
        "idx_to_vocab = {}\n",
        "vocab_size = len(vocab)\n",
        "for idx,v in enumerate(vocab):\n",
        "    vocab_to_idx[v] = idx\n",
        "    idx_to_vocab[idx] = v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "697f756a-e3e4-49a8-8e84-7771e991898e",
      "metadata": {
        "id": "697f756a-e3e4-49a8-8e84-7771e991898e"
      },
      "outputs": [],
      "source": [
        "tokens_num = []\n",
        "for i in tokens:\n",
        "    tokens_num.append(vocab_to_idx[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3372603f-19b1-4fa5-9a96-b1b82fc47500",
      "metadata": {
        "id": "3372603f-19b1-4fa5-9a96-b1b82fc47500"
      },
      "outputs": [],
      "source": [
        "x = []\n",
        "y = []\n",
        "x_num = []\n",
        "y_num = []\n",
        "max_len = 10\n",
        "for i in range(len(tokens) - max_len - 1):\n",
        "    x.append(tokens[i:max_len+i])\n",
        "    y.append(tokens[max_len+i])\n",
        "    x_num.append(tokens_num[i:max_len+i])\n",
        "    y_num.append(tokens_num[max_len+i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dfa428b1-b826-42ff-96ec-3e8fe3ee9c91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfa428b1-b826-42ff-96ec-3e8fe3ee9c91",
        "outputId": "c4e13418-002a-4269-8632-33808854cd5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak']\n",
            "\n",
            "['citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '']\n",
            "all\n",
            "['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', 'all']\n",
            "speak\n",
            "['we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', 'all', 'speak']\n",
            "speak\n",
            "['proceed', 'any', 'further', 'hear', 'me', 'speak', '', 'all', 'speak', 'speak']\n",
            "\n",
            "['any', 'further', 'hear', 'me', 'speak', '', 'all', 'speak', 'speak', '']\n",
            "first\n",
            "['further', 'hear', 'me', 'speak', '', 'all', 'speak', 'speak', '', 'first']\n",
            "citizen\n",
            "['hear', 'me', 'speak', '', 'all', 'speak', 'speak', '', 'first', 'citizen']\n",
            "you\n",
            "['me', 'speak', '', 'all', 'speak', 'speak', '', 'first', 'citizen', 'you']\n",
            "are\n",
            "['speak', '', 'all', 'speak', 'speak', '', 'first', 'citizen', 'you', 'are']\n",
            "all\n",
            "[1342, 3242, 6043, 5774, 544, 4911, 2383, 6651, 591, 1772]\n",
            "0\n",
            "[3242, 6043, 5774, 544, 4911, 2383, 6651, 591, 1772, 0]\n",
            "382\n",
            "[6043, 5774, 544, 4911, 2383, 6651, 591, 1772, 0, 382]\n",
            "1772\n",
            "[5774, 544, 4911, 2383, 6651, 591, 1772, 0, 382, 1772]\n",
            "1772\n",
            "[544, 4911, 2383, 6651, 591, 1772, 0, 382, 1772, 1772]\n",
            "0\n",
            "[4911, 2383, 6651, 591, 1772, 0, 382, 1772, 1772, 0]\n",
            "1342\n",
            "[2383, 6651, 591, 1772, 0, 382, 1772, 1772, 0, 1342]\n",
            "3242\n",
            "[6651, 591, 1772, 0, 382, 1772, 1772, 0, 1342, 3242]\n",
            "6031\n",
            "[591, 1772, 0, 382, 1772, 1772, 0, 1342, 3242, 6031]\n",
            "5277\n",
            "[1772, 0, 382, 1772, 1772, 0, 1342, 3242, 6031, 5277]\n",
            "382\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print(x[i])\n",
        "    print(y[i])\n",
        "for i in range(10):\n",
        "    print(x_num[i])\n",
        "    print(y_num[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6e5821fc-f26d-43d3-ad1c-396410330255",
      "metadata": {
        "id": "6e5821fc-f26d-43d3-ad1c-396410330255"
      },
      "outputs": [],
      "source": [
        "dmodel = 512\n",
        "heads = 4\n",
        "batch_size = 32\n",
        "max_len = 10\n",
        "shape = (batch_size,max_len,dmodel)\n",
        "sentence = torch.Tensor(x_num).long()\n",
        "label = torch.Tensor(y_num).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "642e7702-4c53-4978-a74d-09f50c65802f",
      "metadata": {
        "id": "642e7702-4c53-4978-a74d-09f50c65802f"
      },
      "outputs": [],
      "source": [
        "batch = []\n",
        "for i in range(sentence.shape[0]//32):\n",
        "    if i == 0:\n",
        "        batch.append([sentence[0:32],label[0:32]])\n",
        "    else:\n",
        "        batch.append([sentence[i*32:(i+1)*32],label[i*32:(i+1)*32]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2e3f3c3b-2c07-4e01-b76c-25153e5a7143",
      "metadata": {
        "id": "2e3f3c3b-2c07-4e01-b76c-25153e5a7143"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    '''\n",
        "    Converts the vector embedding of a batch of sequences to their positional encoding vectors.\n",
        "\n",
        "    Arguments:\n",
        "            shape : shape of embedding vector => tuple(batch_size, max_len, dmodel)\n",
        "            device : device to perform the computation on (e.g., 'cpu' or 'cuda')\n",
        "\n",
        "    Returns:\n",
        "            positional encoded vector\n",
        "\n",
        "    '''\n",
        "    def __init__(self, shape, device='cpu'):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.max_len = shape[1]\n",
        "        self.dmodel = shape[2]\n",
        "        self.device = device\n",
        "\n",
        "        position = torch.arange(0, self.max_len, device=self.device).float().unsqueeze(1)        \n",
        "        \n",
        "        div_term = torch.exp(torch.arange(0, self.dmodel, 2, device=self.device).float() * -(math.log(10000.0) / self.dmodel))\n",
        "\n",
        "        pos_enc = torch.zeros((1, self.max_len, self.dmodel), device=self.device)\n",
        "        pos_enc[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pos_enc[0, :, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.pos_enc = pos_enc\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_enc[:, :x.size(1), :]\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2738ce16-3467-4300-b3bb-92442eadaca1",
      "metadata": {
        "id": "2738ce16-3467-4300-b3bb-92442eadaca1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    '''\n",
        "    Multi-Head Attention mechanism for transformer models.\n",
        "\n",
        "    Arguments:\n",
        "        dmodel: Dimension of the model\n",
        "        heads: Number of attention heads\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Perform multi-head attention on the input tensor x\n",
        "    '''\n",
        "    def __init__(self, dmodel, heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.dmodel = dmodel\n",
        "        self.heads = heads\n",
        "        self.head_size = dmodel // heads\n",
        "\n",
        "        self.k_linear = nn.Linear(dmodel, dmodel)\n",
        "        self.q_linear = nn.Linear(dmodel, dmodel)\n",
        "        self.v_linear = nn.Linear(dmodel, dmodel)\n",
        "        self.out_linear = nn.Linear(dmodel, dmodel)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        '''\n",
        "        Split the last dimension into (heads, head_size) and transpose to shape (batch_size, heads, seq_len, head_size).\n",
        "        '''\n",
        "        return x.view(batch_size, -1, self.heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "    def attention(self, k, q, v):\n",
        "        '''\n",
        "        Compute the attention weights and apply them to the value vectors.\n",
        "        '''\n",
        "        d_k = q.size(-1)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32, device=q.device))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        return torch.matmul(attn, v)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Perform the multi-head attention mechanism on the input tensor x.\n",
        "        '''\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        K = self.split_heads(self.k_linear(x), batch_size)  # Key: What can I offer\n",
        "        Q = self.split_heads(self.q_linear(x), batch_size)  # Query: What am I looking for\n",
        "        V = self.split_heads(self.v_linear(x), batch_size)  # Value: What I actually offer\n",
        "\n",
        "        attn_output = self.attention(K, Q, V)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.dmodel)\n",
        "        \n",
        "        return self.out_linear(attn_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fd3a1554-fac1-45a2-a86a-9d817e7b0eb3",
      "metadata": {
        "id": "fd3a1554-fac1-45a2-a86a-9d817e7b0eb3"
      },
      "outputs": [],
      "source": [
        "class AddAndNorm(nn.Module):\n",
        "    '''\n",
        "    Add and Layer Normalization module for transformer models.\n",
        "\n",
        "    Arguments:\n",
        "        dmodel: Dimension of the model\n",
        "\n",
        "    Methods:\n",
        "        forward(x, residual): Add the input tensor x and the residual tensor, then apply layer normalization\n",
        "    '''\n",
        "    def __init__(self, dmodel):\n",
        "        super(AddAndNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(dmodel)\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        '''\n",
        "        Add the input tensor x and the residual tensor, then apply layer normalization.\n",
        "\n",
        "        Arguments:\n",
        "            x: Input tensor\n",
        "            residual: Residual tensor to be added to the input tensor\n",
        "\n",
        "        Returns:\n",
        "            Tensor after addition and layer normalization\n",
        "        '''\n",
        "        return self.layer_norm(x + residual)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3abf7b27-898e-4eca-a720-1da2abea3419",
      "metadata": {
        "id": "3abf7b27-898e-4eca-a720-1da2abea3419"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    '''\n",
        "    Position-wise Feed-Forward Network for transformer models with dropout.\n",
        "\n",
        "    Arguments:\n",
        "        dmodel: Dimension of the model\n",
        "        dropout: Dropout probability\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Apply the feed-forward network with dropout on the input tensor x\n",
        "    '''\n",
        "    def __init__(self, dmodel, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(dmodel, dmodel)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dmodel, dmodel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Apply the feed-forward network with dropout on the input tensor x.\n",
        "\n",
        "        Arguments:\n",
        "            x: Input tensor\n",
        "\n",
        "        Returns:\n",
        "            Tensor after applying the feed-forward network and dropout\n",
        "        '''\n",
        "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a997cda3-7168-4fe7-994a-48fb53bd0ed6",
      "metadata": {
        "id": "a997cda3-7168-4fe7-994a-48fb53bd0ed6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Transformer Encoder implementation.\n",
        "\n",
        "    Arguments:\n",
        "        vocab_size: Size of the vocabulary\n",
        "        shape: Shape of the input tensor (batch_size, max_len, dmodel)\n",
        "        heads: Number of attention heads\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Forward pass through the encoder\n",
        "    '''\n",
        "    def __init__(self, vocab_size, shape, heads=4):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, shape[2])\n",
        "        self.positional_encoding = PositionalEncoding(shape)\n",
        "        self.multi_headed_attention = MultiHeadAttention(shape, heads)\n",
        "        self.add_and_norm1 = AddAndNorm(shape[2])\n",
        "        self.feed_forward = FeedForward(dmodel=shape[2])\n",
        "        self.add_and_norm2 = AddAndNorm(shape[2])\n",
        "        self.linear = nn.Linear(shape[2], vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.token_embedding_table(x)\n",
        "        residual = self.positional_encoding(out)\n",
        "        out = self.multi_headed_attention(residual)\n",
        "\n",
        "        residual = self.add_and_norm1(out, residual)\n",
        "        out = self.feed_forward(residual)\n",
        "        out = self.add_and_norm2(out, residual)\n",
        "\n",
        "        out = self.linear(out)\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fed33ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Pretraining(nn.Module):\n",
        "    '''\n",
        "    Pretraining model for next word prediction using a transformer encoder.\n",
        "\n",
        "    Arguments:\n",
        "        vocab_size: Size of the vocabulary\n",
        "        shape: Shape of the input tensor (batch_size, max_len, dmodel)\n",
        "        heads: Number of attention heads\n",
        "\n",
        "    Methods:\n",
        "        forward(x): Forward pass through the pretraining model\n",
        "        predict_next_word(x): Predict the next word for the input sequence\n",
        "    '''\n",
        "    def __init__(self, vocab_size, shape, heads=4):\n",
        "        super(Pretraining, self).__init__()\n",
        "        self.encoder = Encoder(vocab_size, shape, heads)\n",
        "        \n",
        "        self.linear = nn.Linear(shape[2], vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encoder(x)\n",
        "        out = self.linear(out[:, -1, :])  # Use the last token's embedding\n",
        "        out = self.softmax(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5b97bbdd-127d-495e-b32c-ab099312f78b",
      "metadata": {
        "id": "5b97bbdd-127d-495e-b32c-ab099312f78b"
      },
      "outputs": [],
      "source": [
        "model = Encoder(vocab_size,shape)\n",
        "criterition = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "import random\n",
        "random.shuffle(batch)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2547a554-a919-416f-b263-58eca9234031",
      "metadata": {
        "id": "2547a554-a919-416f-b263-58eca9234031"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6259 [00:06<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Convert targets to one-hot encoding\u001b[39;00m\n\u001b[0;32m     15\u001b[0m targets_one_hot \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(targets, num_classes\u001b[38;5;241m=\u001b[39mvocab_size)\u001b[38;5;241m.\u001b[39mfloat()\n",
            "File \u001b[1;32mc:\\Users\\kunalkushwahatg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\kunalkushwahatg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[23], line 16\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m     15\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table(x)\n\u001b[1;32m---> 16\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_headed_attention(residual)\n\u001b[0;32m     19\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_and_norm1(out,residual,)\n",
            "File \u001b[1;32mc:\\Users\\kunalkushwahatg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\kunalkushwahatg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[11], line 35\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m pos_enc[:, :, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(position \u001b[38;5;241m*\u001b[39m div_term)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Add positional encoding to the input embeddings\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpos_enc\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Return the positional encoded vector\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    losses = []\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for b in tqdm.tqdm(batch):\n",
        "        inputs, targets = b[0].to(device), b[1].to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(inputs)\n",
        "\n",
        "        # Convert targets to one-hot encoding\n",
        "        targets_one_hot = F.one_hot(targets, num_classes=vocab_size).float()\n",
        "\n",
        "        loss = criterition(out ,targets_one_hot)\n",
        "\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        losses.append(loss)\n",
        "\n",
        "    # Calculate and print the average loss for the epoch\n",
        "    average_loss = running_loss / len(batch)\n",
        "    print(f'Epoch {epoch + 1}, Average Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c69d0b",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dL8uebhrk9Og",
      "metadata": {
        "id": "dL8uebhrk9Og"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fml8-Hy9s_Jj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fml8-Hy9s_Jj",
        "outputId": "78d72df5-8ad0-4b70-b5d8-606d6844d806"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(8.8180, device='cuda:0', grad_fn=<DivBackward1>)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1sZE6vQ1tKMy",
      "metadata": {
        "id": "1sZE6vQ1tKMy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
